{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405/DIT407 Introduction to Data Science and AI \n",
    "## 2022-2023, Reading Period 4\n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "The exercise takes place in this notebook environment.\n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use \n",
    "7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n",
    "**What to submit:** \n",
    "Convert the notebook to a pdf-file and submit it. Make sure all cells are executed so all your code and its results are included. Double check the pdf displays correctly before you submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [],
   "source": [
    "#Download and extract data\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "!tar -xjf 20021010_easy_ham.tar.bz2\n",
    "!tar -xjf 20021010_hard_ham.tar.bz2\n",
    "!tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "### 1. Preprocessing: \n",
    "\n",
    "##### 1.1 Look at a few emails from easy_ham, hard_ham and spam. Do you think you would be able to classify the emails just by inspection? How do you think a succesful model can learn the difference between the different classes of emails?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nobel Honors 3 for Astrophysics Work \n",
      "\n",
      "Tuesday October 8, 2002 12:00 PM\n",
      "STOCKHOLM, Sweden (AP) - Two Americans and a Japanese won the Nobel Prize in\n",
      "physics Tuesday for using some of the most obscure particles and waves in\n",
      "nature to understand the workings of astronomy's grandest wonders.\n",
      "Riccardo Giacconi, 71, of the Associated Universities Inc. in Washington,\n",
      "D.C., will get half of the $1 million prize for his role in ``pioneering\n",
      "contributions to astrophysics, which have led to the discovery of cosmic\n",
      "X-ray sources.''\n",
      "Raymond Davis, Jr., 87, of the University of Pennsylvania shares the other\n",
      "half of the prize with Japanese scientist Masatoshi Koshiba, 76, of the\n",
      "University of Tokyo. The two men pioneered the construction of giant\n",
      "underground chambers to detect neutrinos, elusive particles that stream from\n",
      "the sun by the billion.\n",
      "Neutrinos offer an unparalleled view of the sun's inner workings because\n",
      "they are produced in its heart by the same process that causes it to shine.\n",
      "In fact, Davis' early experiments, performed during the 1960s in a South\n",
      "Dakota gold mine, confirmed that the sun is powered by nuclear fusion.\n",
      "Koshiba won his share of the prize for his work at the Kamiokande neutrino\n",
      "detector in Japan. That experiment confirmed and extended Davis' work, and\n",
      "also discovered neutrinos coming from distant supernova explosions, some of\n",
      "the brightest objects in the universe.\n",
      "The Italian-born Giacconi, a U.S. citizen, was awarded half of the prize for\n",
      "building the first X-ray telescopes that provided ``completely new - and\n",
      "sharp - images of the universe,'' the academy said.\n",
      "His research laid the foundation for X-ray astronomy, which has led to the\n",
      "discovery of black holes and allowed researchers to peer deep into the\n",
      "hearts of the dusty young galaxies where stars are born.\n",
      "When academy officials reached Giacconi by phone at his home outside\n",
      "Washington, he said he was ``dumbstruck'' to learn of the prize. Koshiba\n",
      "also was phoned at home in Tokyo, but the academy was still trying to reach\n",
      "Davis, spokesman Erling Norrby said.\n",
      "This year's Nobel awards started Monday with the naming of Britons Sydney\n",
      "Brenner, 75, and Sir John E. Sulston, 60, and American H. Robert Horvitz,\n",
      "55, as winners of the medicine prize, selected by a committee at the\n",
      "Karolinska Institute.\n",
      "The researchers shared it for discoveries about how genes regulate organ\n",
      "growth and a process of programmed cell deaths that shed light on how\n",
      "viruses and bacteria invade human cells, including in conditions such as\n",
      "AIDS, strokes, cancer and heart attacks.\n",
      "The winner of the Nobel Prize in chemistry will be named on Wednesday\n",
      "morning and the Bank of Sweden Prize in Economic Sciences in Memory of\n",
      "Alfred Nobel later the same day.\n",
      "The literature prize winner will be announced on Thursday, the Swedish\n",
      "Academy said on Tuesday.\n",
      "The winner of the coveted peace prize - the only one not awarded in Sweden -\n",
      "will be announced Friday in Oslo, Norway.\n",
      "The award committees make their decisions in deep secrecy and candidates are\n",
      "not publicly revealed for 50 years.\n",
      "Alfred Nobel, the wealthy Swedish industrialist and inventor of dynamite who\n",
      "endowed the prizes left only vague guidelines for the selection committees.\n",
      "In his will he said the prize being revealed on Tuesday should be given to\n",
      "those who ``shall have conferred the greatest benefit on mankind'' and\n",
      "``shall have made the most important discovery or invention within the field\n",
      "of physics.''\n",
      "The Royal Swedish Academy of Sciences, which also chooses the chemistry and\n",
      "economics winners, invited nominations from previous recipients and experts\n",
      "in the fields before cutting down its choices. Deliberations are conducted\n",
      "in strict secrecy.\n",
      "The prizes are presented on Dec. 10, the anniversary of Nobel's death in\n",
      "1896, in Stockholm and in Oslo.\n",
      "---\n",
      "On the Net:\n",
      "Nobel site, http://www.nobel.se \n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "Plan to Sell a Home?\n",
      "http://us.click.yahoo.com/J2SnNA/y.lEAA/MVfIAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code for here for looking a few emails\n",
    "import os, random\n",
    "\n",
    "ham = random.choice(os.listdir(\"./easy_ham/\"))\n",
    "spam = random.choice(os.listdir(\"./spam/\"))\n",
    "\n",
    "#print(fetch_format(f\"./spam/{spam}\"))\n",
    "print(fetch_format(f\"./easy_ham/{ham}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1.1:\n",
    "It seems feasible to classify the emails only from inspection. It seems like a lot of spam mails contains words like \"invest\", \"click\" and \"100%\" etcetera. To create a model for this we could generate some statistics on which words often appear in spam mails but seldom in ham mails. Then we would know which words to look for. Looking through a mail we could generate a score based on how many of these words appear and taking into account how statistically probable they are to be in a spam mail. Another thing to look for is HTML tags. It seems a lot of spam mails contains HTML whereas ham mails doesn't. Simply looking for HTML-tags would suffice for this since that is needed for any HTML to work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Note that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text (in the optional part further down can experiment with filtering out the headers and footers). We don’t want to train and test on the same data (it might help to reflect on why if you don't recall). Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`). Use only the easy_ham part as ham data for quesions 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for here for splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ham_paths = os.listdir(\"./easy_ham/\")\n",
    "spam_paths = os.listdir(\"./spam/\")\n",
    "\n",
    "hamtrain_paths, hamtest_paths = train_test_split(ham_paths, test_size=0.3)\n",
    "spamtrain_paths, spamtest_paths = train_test_split(spam_paths, test_size=0.3)\n",
    "\n",
    "hamtrain = [fetch_format(f\"./easy_ham/{x}\") for x in hamtrain_paths]\n",
    "hamtest = [fetch_format(f\"./easy_ham/{x}\") for x in hamtest_paths]\n",
    "spamtrain = [fetch_format(f\"./spam/{x}\") for x in spamtrain_paths]\n",
    "spamtest = [fetch_format(f\"./spam/{x}\") for x in spamtest_paths]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "### 2.1 Write a Python program that: \n",
    "1.\tUses the four datasets from Question 1 (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (use the [scikit-learn library](https://scikit-learn.org/stable/)) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. Use `CountVectorizer` ([Documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)) to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in scikit-learn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem:\n",
    "- Multinomial Naive Bayes  \n",
    "- Bernoulli Naive Bayes. \n",
    "\n",
    "Please inspect the documentation to ensure input to the classifiers is appropriate before you start coding. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1785, 12511450]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m htr_vector \u001b[38;5;241m=\u001b[39m ham_vectorizer\u001b[38;5;241m.\u001b[39mtransform(hamtrain)\n\u001b[1;32m     14\u001b[0m str_vector \u001b[38;5;241m=\u001b[39m spam_vectorizer\u001b[38;5;241m.\u001b[39mtransform(spamtrain)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtr_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gnb\u001b[38;5;241m.\u001b[39mpredict(hamtest)\n",
      "File \u001b[0;32m~/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py:267\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=264'>265</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=265'>266</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(y\u001b[39m=\u001b[39my)\n\u001b[0;32m--> <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=266'>267</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=267'>268</a>\u001b[0m     X, y, np\u001b[39m.\u001b[39;49munique(y), _refit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=268'>269</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py:428\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=424'>425</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=426'>427</a>\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[0;32m--> <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=427'>428</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=428'>429</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/naive_bayes.py?line=429'>430</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py?line=582'>583</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py?line=584'>585</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/base.py?line=586'>587</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1105'>1106</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1106'>1107</a>\u001b[0m     X,\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1107'>1108</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1118'>1119</a>\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1119'>1120</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1121'>1122</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1123'>1124</a>\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1125'>1126</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=394'>395</a>\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=395'>396</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=396'>397</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=397'>398</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=398'>399</a>\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    <a href='file:///home/hannes/Plugg/DAT405/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py?line=399'>400</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1785, 12511450]"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "ham_vectorizer = CountVectorizer()\n",
    "ham_vectorizer.fit(hamtrain)\n",
    "spam_vectorizer = CountVectorizer()\n",
    "spam_vectorizer.fit(spamtrain)\n",
    "\n",
    "htr_vector = ham_vectorizer.transform(hamtrain)\n",
    "str_vector = spam_vectorizer.transform(spamtrain)\n",
    "\n",
    "gnb.fit(htr_vector.toarray(), str_vector.toarray().reshape(-1,1))\n",
    "y_pred = gnb.predict(hamtest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Answer the following questions:\n",
    "##### a) What does the CountVectorizer do?\n",
    "Answer 2.2.a\n",
    "##### b) What is the difference between Multinomial Naive Bayes and Bernoulli Naive Bayes\n",
    "Answer 2.2.b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.1 Run the two models:\n",
    "Run (don't retrain) the two models from Question 2 on spam versus hard-ham. Does the performance differ compared to question 2 when the model was run on spam versus easy-ham? If so, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gool_zb8Qzzy"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Retrain\n",
    "Retrain new Multinomial and Bernolli Naive Bayes classifers on the combined (easy+hard) ham and spam. Now evaluate on spam versus hard-ham as in 3.1. Also evaluate on spam versus easy-ham. Compare the performance with question 2 and 3.1. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.2:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Further improvements\n",
    "Do you have any suggestions for how performance could be further improved? You don't have to implement them, just present your ideas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching text from a file path and removing the header info\n",
    "def fetch_format(path):\n",
    "   with open(path, 'r', encoding='latin1') as f:\n",
    "      split = f.read().split('\\n\\n', 1)\n",
    "   if len(split) == 1:\n",
    "      return split[0]\n",
    "   return split[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "75df9f73f605be1caca45e667bef8c70565b1e5682af74864e8b0867b6079d60"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
